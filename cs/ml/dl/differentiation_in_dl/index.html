<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Differentiation in Deep Learning</title> <header> <div class=blog-name ><a href="/">phunc20</a></div> <nav> <ul> <li><a href="/">Home</a> <li><a href="/posts">Posts</a> <li><a href="/about/">About</a> <li><a href="/tags/">Tags</a> <li><a href="/support_me/">Support Me</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=differentiation_in_deep_learning ><a href="#differentiation_in_deep_learning" class=header-anchor >Differentiation in Deep Learning</a></h1> <div class=tags ><a href="/tags/" id=tag-icon ><svg width=20  height=20  viewBox="0 0 512 512"><defs><style>.cls-1{fill:#141f38}</style></defs><path class=cls-1  d="M215.8 512a76.1 76.1 0 0 1-54.17-22.44L22.44 350.37a76.59 76.59 0 0 1 0-108.32L242 22.44A76.11 76.11 0 0 1 296.2 0h139.2A76.69 76.69 0 0 1 512 76.6v139.19A76.08 76.08 0 0 1 489.56 270L270 489.56A76.09 76.09 0 0 1 215.8 512zm80.4-486.4a50.69 50.69 0 0 0-36.06 14.94l-219.6 219.6a51 51 0 0 0 0 72.13l139.19 139.19a51 51 0 0 0 72.13 0l219.6-219.61a50.67 50.67 0 0 0 14.94-36.06V76.6a51.06 51.06 0 0 0-51-51zm126.44 102.08A38.32 38.32 0 1 1 461 89.36a38.37 38.37 0 0 1-38.36 38.32zm0-51a12.72 12.72 0 1 0 12.72 12.72 12.73 12.73 0 0 0-12.72-12.76z"/><path class=cls-1  d="M217.56 422.4a44.61 44.61 0 0 1-31.76-13.16l-83-83a45 45 0 0 1 0-63.52L211.49 154a44.91 44.91 0 0 1 63.51 0l83 83a45 45 0 0 1 0 63.52L249.31 409.24a44.59 44.59 0 0 1-31.75 13.16zm-96.7-141.61a19.34 19.34 0 0 0 0 27.32l83 83a19.77 19.77 0 0 0 27.31 0l108.77-108.7a19.34 19.34 0 0 0 0-27.32l-83-83a19.77 19.77 0 0 0-27.31 0l-108.77 108.7z"/><path class=cls-1  d="M294.4 281.6a12.75 12.75 0 0 1-9-3.75l-51.2-51.2a12.8 12.8 0 0 1 18.1-18.1l51.2 51.2a12.8 12.8 0 0 1-9.05 21.85zM256 320a12.75 12.75 0 0 1-9.05-3.75l-51.2-51.2a12.8 12.8 0 0 1 18.1-18.1l51.2 51.2A12.8 12.8 0 0 1 256 320zM217.6 358.4a12.75 12.75 0 0 1-9-3.75l-51.2-51.2a12.8 12.8 0 1 1 18.1-18.1l51.2 51.2a12.8 12.8 0 0 1-9.05 21.85z"/></svg></a><a href="/tag/deep_learning/">deep learning</a></div> <p>When I started to learn Neural Networks, people kept emphasizing the importance of Back Propagation, that it worths to thoroughly understand it, that one could not fully grasp and utilise Neural Networks without it, etc. On the other hand, coding in Deep Learning frameworks like TensorFlow or PyTorch does not give the feeling that Back Propagation is that indispensible.</p> <p>At that time, I took these remarks seriously and read or watched a whole bunch of videos, books and articles on Backprop. But I always had had an uneased feeling about Backprop.</p> <p>With hindsight, the single most used differentiation technique in Deep Learning is gradient descent, which is taught to almost everyone who has a university degree and which was discovered long long ago in the history of Mathematics. So what novelty is there in Deep Learning that has to wait until the 21st century?</p> <h3 id=autodiff ><a href="#autodiff" class=header-anchor >Autodiff</a></h3> <p>To me, the novelty is automatic differentiation. It is the way computers nowadays efficiently calculate the differential of a function &#40;when either or both the dimensions of the domain and the codomain is large&#41;.</p> <p>As for Back Propagation, I think what is important is that one really takes a specific example &#40;e.g. a shallow NN, CNN, RNN, etc.&#41;, and figures out the following</p> <ul> <li><p>with respect to which variables the gradient is computed</p> <li><p>despite the length and complexity of calculation, how to systematically and correctly carry it out</p> <li><p>subgradient</p> <ul> <li><p>Admittedly, I myself only start to notice this concept since year 2022. I plan to write a new post exclusively on it, a draft of which can be found at <a href="/cs/ml/dl/subgradient/ez_viz_with_NN.jl"><strong>here</strong></a></p> <li><p>There seems to be many resources introducing subgradient and related methods for non differentiable function optimization, one of which I find particularly good was <a href="http://www.seas.ucla.edu/~vandenbe/ee236c.html">Lieven Vandenberghe&#39;s handouts</a></p> <li><p>How subgradients can be taken into consideration under the framework of autodiff is also a question worth answering. That is, how deep learning frameworks &#40;TensorFlow, PyTorch, Flux, etc.&#41; really minimize their loss function when the loss function is not differentiable.</p> </ul> </ul> <p>Below are a few materials that I particularly recommend:</p> <ul> <li><p>Autodiff</p> <ul> <li><p><a href="https://www.youtube.com/watch?v&#61;rZS2LGiurKY&amp;t&#61;1356s">Alan Edelman</a></p> <ul> <li><p>In case you could not manage to find the Juypter notebook that Professor Edelman used in the video, you may try <a href="https://github.com/phunc20/youtube/blob/master/mit/18.065/spring2018/36-alan_edelman_and_julia/autodiff.ipynb">this one </a> on my github</p> </ul> <li><p><a href="https://www.youtube.com/watch?v&#61;wG_nF1awSSY">Ari Seff</a></p> </ul> <li><p>Backprop</p> <ul> <li><p><a href="/cs/ml/dl/en_nn_andrew_ng.pdf"><em>Backprop: A Simple Case</em></a></p> <p>This is an article I wrote independently after my first job as a data scientist and before my second one. Chronologically, I only discovered the wonderful topic of Autodiff months later.</p> </ul> </ul> <p>I have mentioned earlier that I had had an uneased feeling about Backprop. By sitting down and scratching the ideas written in the article, I have realized that Backprop is just a fancy word. It&#39;s nothing but ordinary differentiation. What really matters is that one really carries out an example&#39;s calculation and convince themselves that they have understood.</p> <p>Even after finishing editing the article in LaTeX, the uneased feeling did not disappear. I thought,</p> <blockquote> <p>&quot;Now, theoretically I can carry out the computations in similar examples, but how on Earth should I command a computer to do that?&quot;</p> </blockquote> <p>Indeed, it&#39;s simply too complicated if a computer has to go that way. And that&#39;s where Autodiff comes up on stage.</p> <h3 id=flux ><a href="#flux" class=header-anchor >Flux</a></h3> <p>TensorFlow, PyTorch, Flux and most Deep Learning frameworks implements autodiff. The readers of this post could choose any one of them and start to play with it.</p> <p>It&#39;s also very educative to introduce these frameworks to those who starts to learn Calculus.</p> <p>I have started to write a tutorial of my own on how to use Flux, a Julia DL framework. In particular, the following article demonstrates how Autodiff could be easily done in such frameworks: <a href="/cs/julia/Flux/tuto/gradient_jacobien.jl"><code>gradient</code> function in Flux</a></p> <div class=page-foot > <div class=copyright > &copy; Septimia Zenobia. Last modified: September 09, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>. </div> </div> </div>